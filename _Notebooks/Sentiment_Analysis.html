---
title : Movie Recommender
permalink : /Projects/:title
---
<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/css/bootstrap.min.css" rel="stylesheet" id="bootstrap-css">
<script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
<link rel="stylesheet" type="text/css" href="../css/nav.css">
 <link rel="stylesheet" href="../css/blog.css">   
<link href="https://fonts.googleapis.com/css?family=Roboto+Condensed" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
<link rel="stylesheet" type="text/css" href="../css/tango.css">
<!------ Include the above in your HEAD tag ---------->

    <div id="wrapper">
        <div class="overlay"></div>
    
        <!-- Sidebar -->
   {% include sidebar.html %}
        <!-- /#sidebar-wrapper -->

        <!-- Page Content -->
        <div id="page-content-wrapper">
            <button type="button" class="hamburger is-closed" data-toggle="offcanvas">
                <span class="hamb-top"></span>
                <span class="hamb-middle"></span>
                <span class="hamb-bottom"></span>
            </button>
            <div class="container-fluid">
                <div class="row">
                    <div style="padding: 0px;" class="col-lg-12">
                       <div style="height: 50%;width: 100%;position: relative; " class="image_overrelay">
<div class="heading_container"><h1  class="text-center" style="font-family: 'Roboto Condensed';
    color: white;
    font-size: xx-large;
    text-align: left;
    margin: 10%;">Movie Reviews Sentiment Analysis Using Machine Learning</h1>
</div>
<img class="image-banner" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRkXpmvxmxrmY5LIqh2Q4NOGdkbpBo-3SsXEGwiF5s0tkiwoqAY">  

</div>
                   
                    </div>
                </div>
<div style="
    margin: 4% 3%;
    text-align: center;
    font-size: large;
    color: #909192;
">
    skills : Natural Language Processing,Scikit-learn,python,Random Forest,Naive Bayes
</div>
                 <div class="row">
                    <div class="col-md-8 col-md-offset-2 col-sm-8 col-md-offset-2 col-xs-12 ">

<hr>
<h2>About the Project</h2>
In this Project we will be building a Sentiment Analyzer that will be used to judge sentiment of movie reviews.
The dataset has been taken from the rotten tomatoes website.We will be dealing will Natural language processing in python and will be sung several modules of python for NLP.This notebook aims at cleaning data as we are dealing with raw reviews.Lets start
<h2>Exploration and Analysis</h2>
<p>Lets import our libraries</p>


{% highlight python %}

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

{% endhighlight %}        

<p>we are having only one data file <code>train.tsv</code> . Now tsv(tab seprate values) are another extension of data files just like .xls or .csv. Pandas has a special function read_table which allows us to deal with these files/<p>

<p>lets see what we have in these files</p>


{% highlight python %}
data =  pd.read_table('train.tsv')
data.head(5)  
{% endhighlight %}      

<table style="overflow-x:scroll;"  border="1" class="table table-responsive">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PhraseId</th>
      <th>SentenceId</th>
      <th>Phrase</th>
      <th>Sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>A series of escapades demonstrating the adage ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>A series of escapades demonstrating the adage ...</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>A series</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>A</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>1</td>
      <td>series</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>The Phrase has all the Sentiments of the reviews.The reviews have been parsed by the stanford parser and split into sentences. Each review has a <code>sentence id</code> and a <code>phrase id</code>.
</p>

<p> Now if we observe the sentiments columns we have multiclass labels to sentiments.sentiments have 0,1,2,3,4 numeric classes.Tne dataset source stated that what each numeric label meant so </p>

<p>
<code>0 </code> : Negative <br>   
<code>1 </code> : Somewhat Negative <br>
<code>2 </code> : Neutral <br>
<code>3 </code> : Positive <br>
<code>4 </code> : Somewhat Postive <br>
</p>

<p>For visualization purpose lets convert the numeric columns to categorical.Lets write a method to convert each row of pandas datadframe to numerical</p>

{% highlight python %}
def num_sen(x):
    if(x == 0):
        return "negative"
    if(x == 1):
        return "somewhat negative"
    if(x == 2):
        return "neutral"
    if(x == 3):
        return "positive"
    if(x==4):
        return "somewhat positive"        
{% endhighlight %}          

<p>Now we will analyze the distrbution of various sentiments of dataset</p>

{% highlight python %}
data['Sentiment'] = data['Sentiment'].map(num_sen)
sns.countplot(y=data.Sentiment)
{% endhighlight %}        


<img src="../static_assets/Sentiment Analysis/count.png" alt="">


<p>Looks like we have a lot of samples belonging to neutral class :-).People try to be Diplomatic in film reviews</p>

<hr>

<h3>Postive Sentiments Analysis</h3>

<p>In this Section we will focus only on the Postive Sentiments from the dataset.We will breakdown sentiments into words,Clean the data and visualize.</p>

<p>Lets extract the positive sentiments out of the Sentiments column </p>

{% highlight python %}

df_sentiments = data[data['Sentiment'] == "positive"]

{% endhighlight %}        

<p>Now we will split each sentence into tokens of words.</p>

{% highlight python %}

positive = df_sentiments['Phrase'].apply(lambda x : x.split(' '))

{% endhighlight %}   


<p>After this we will count the words which occour in positive sentiments and their counts.this will help us understand what positive sentence mainly comprises of and what do we actually have in them.</p>     


{% highlight python %}
positive_dict = dict()
for words in positive:
    for word in words:
        if word not in positive_dict.keys():
            positive_dict[word] = 1
        else:
            positive_dict[word] = positive_dict[word]+1

{% endhighlight %}  

<p>Framing the results into dataset for visualization.</p>      

{% highlight python %}

positive_counts = pd.DataFrame([[x,positive_dict[x]] for x in positive_dict],columns=['word','count'])

{% endhighlight %}

<p>Lets do a barplot</p>
{% highlight python %}
fig = plt.figure(figsize=(12,18))
sns.barplot(y = positive_counts['word'][:100],x=positive_counts['count'][:100])
{% endhighlight %}        

<img src="../static_assets/Sentiment Analysis/positive_count.png" alt="">

<p>It seems like the most common word or element or token is <code>,</code> followed by <code>the</code> and other 
<b>articles</b>,<<b>pos(parts of speech)</b>.It seems pretty obvious because these comprises of most of what we speak in daily life. <code>The</code> and <code>and</code> are the most common words in English Sentences.
</p>

<p>But regarding building prective model for sentiments analysis these wont play a special role as they would be common to all sentences.</p>

<h3>Cleaning the Positive reviews</h3>

<p>Now its time to gear up and clean our reviews. we have seen in above visualization that these are prominent in our reviews and dont play much important role.we would be cleaning reviews using the python Natural Language Processing library <code>NLTK</code></p>

{% highlight python %}
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
set(stopwords.words('english'))
nltk.download('wordnet')
{% endhighlight %}        

<p>we have imported stopwords Stemmer Lemmatizer and downloaded the wordnet for Lemmatizer.we will see what each of these are and how they will clean our reviews</p>

<ul>
    <li>
        <b>Stopwords Punctuations and special characters</b>
        <p>Stopwords are words which are very repetetive and useless.These words means nothing, unless of course we're searching for someone who is maybe lacking confidence, is confused, or hasn't practiced much speaking. We all do it, you can hear me saying "umm" or "uhh" </p>


    </li>
    <li> <b>Stemming</b>
        <p>Stemming is the process of reducing a word into its stem, i.e. its root form.root form of eating is eat,sleeping is sleep.converting words to roots reduces ambiguity in  sentences</p>
    </li>
    <li><b>Lemmatizing</b>
        <p>Lemmatization is somewhat similar to stemmming but it maps several differnt words to a common root</p>
        <p>for eg: go,going,went are mapped to go</p>
    </li>
</ul>

<p>Now we will write a function that does this for us . Since Both Stemming and Lemmatization reduces words into their root forms , any one can be used.I will be using Lemmatization in this Notebook.we will also use python inbuilt regular expression library <code>re</code> for removing symbols and special characters and Numbers.</p>


{% highlight python %}
import re
# lets define a function that does this 
def clean_reviews(review):
    # remoing the punctuation and special symbols
    review = re.sub('[^A-Za-z]+',' ',review)
    # converting words to lowercase
    review = review.lower()
    # convering sentences to list of words
    review = review.split(' ')
    # removing the stop words
    review_cleaned  = [words for words  in review if words not in stopwords.words('english')] 
    # Stemming
    if(not(Lemma)):
        #stemming words so that we can get into the root word
        stemmer = PorterStemmer()
        review_cleaned = [stemmer.stem(words) for words in review_cleaned]
        return ' '.join([words for words in review_cleaned])
    #Lemmatizing
    else:
        Lemmatizer = WordNetLemmatizer()
        review_cleaned = [Lemmatizer.lemmatize(words) for words in review_cleaned]
        rev =  ' '.join([words for words in review_cleaned])
        return rev
{% endhighlight %}        

<p>The above function processes each of the raw postive raw reviews it the data and return the cleaned review.we have set <code>Lemma = 1</code>so that i could easily switch using the same function</p>

{% highlight python %}
Lemma = 1
{% endhighlight %}        

<p>Now lets gets the things done!!</p>

{% highlight python %}

data['cleaned_reviews'] = data['Phrase'].apply(lambda x : clean_reviews(x))
data['cleaned_reviews']
{% endhighlight %}        
<pre class="output">
 0         series escapade demonstrating adage good goose...
1            series escapade demonstrating adage good goose
2                                                    series
3                                                          
4                                                    series
5                   escapade demonstrating adage good goose
6                                                          
7                   escapade demonstrating adage good goose
8                                                  escapade
9                            demonstrating adage good goose
10                                      demonstrating adage
11                                            demonstrating
12                                                    adage
13                                                         
14                                                    adage
15                                               good goose
16                                                         
17                                               good goose
18                                                         
19                                               good goose
20                                                         
21                                               good goose
22                                                     good
23                                                    goose
24                                                         
25                                                    goose
26                                                    goose
27        also good gander occasionally amuses none amou...
28        also good gander occasionally amuses none amou...
29                                          
 upto no of postive reviews
</pre>



<p>Now we have seen how will be dealing with raw reviews,Lets apply it to the whole sentiments data.</p>
<hr>
<h2>Sentiement Analysis and Predictive Modelling</h2>

<p>Now lets clean all the reviews by applying the <code>clean_reviews</code> to all sentiments and create a new column  in the dataframe 'cleaned_reviews'</p>

{% highlight python %}
data['cleaned_reviews'] = data['Phrase'].apply(lambda x : clean_reviews(x))
data['cleaned_reviews']
{% endhighlight %}        

<p>It takes some 3-5 minutes to process as it has 1,50,000 reviews.Now we will be using our cleaned reviews as feature and start building our predictive model using various NLP techniques</p>

<hr>

<h2>NLP Methods for Text Analysis</h2>
In Natural Language Processing there are several techniques or methods to anayze text data.some of the famous algorthims are <code>Bag of Words</code>,<code>Tfidf(Term Frequency Inverse Document Frequency)</code> and <code>Word2vec</code>.We will be using Bag of Words anf Tfidf for Feature Extraction.Lets see both of the algorithims.

<h3>Bag of Words</h3>
<p>
The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.

The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification.</p>
<h3>TFIDF</h3>
<p>TFIDF(Term frequency inverse document frequency) is also one of the techniques for dealing with text data.In Bag of Words model,we simply count the occourence of words and simply write the count at a partiucular index in the feature vector . But since we are only using count , words which are rare in the reviews will have less impact on predicting the sentiment . TFIDF measures the number of times that words appear in a given document(Term Frequency).The Idf term is the total occourence of a word in all documents to total no of documents in the dataset.</p>
<img src="https://deeplearning4j.org/img/tfidf.png" alt="">

<p>Now we will be buiding a sklearn pipeline to use both the Bag of Words and Tfidf to calculate feature vectors</p>
<hr>
<h2>Building a Sentiment Classifier</h2>


{% highlight python %}
from sklearn.pipleline import pipeline
from sklearn.feature_extraction  import Tfidf,CountVectorizer
{% endhighlight %}        

<p> <code>Tfidf</code> and <code>CountVectorizer</code> are the sklearn implementation of Tfidf and Bag of Words implementation of sklearn. </p>


{% highlight python %}
BOW = CountVectorizer(max_features=5000)
BOW_Features = BOW.fit_transform((data['cleaned_reviews'])) 
{% endhighlight %}     

{% highlight python %}
BOW_Features
{% endhighlight %}

<pre class="output">
<156060x5000 sparse matrix of type '<type 'numpy.int64'>'with 524767 stored elements in Compressed Sparse Row format>
</pre>

<p>We have got a sparse matrix of 156000x5000 wherer each row corresponds to a single document and a feature vector of 5000 features. <code>CountVectorizer</code> allows us choose an arbitrary size of feature vector</p>

<p>Now we will fit a tfidf vectorizer on the bag of words feature</p>


{% highlight python %}
tfidf = Tfidf()
tfidf_features = tfidf.fit_transform(max_features = 5000)
tfidf_features
{% endhighlight %}
<pre class="output">
<156060x2000 sparse matrix of type '<type 'numpy.float64'>'with 419006 stored elements in Compressed Sparse Row format> 
</pre>        

<p>Now we will train a naive bayes classifier for sentiment analysis.Naive Bayes is very popular for text classification and is used popularly.</p>
<hr>
<h2>Train Test Split</h2>

<p>We will split our whole tfidf sparse matrix into train and test using sklearn's <code>train_test_split</code></p>
{% highlight python %}

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, data['Sentiment'], test_size=0.33, random_state=42)
{% endhighlight %}        
<p>Lets train our model on Multinomial Naive Bayes which is used for Multiclassification Problem.</p>


{% highlight python %}
from sklearn.naive_bayes import MultinomialNB()
model_naive = MultinomialNB()
model_naive.fit(X_train,y_train)
{% endhighlight %}        

<p>Now lets predict the values in the test set and see its accuracy</p>

{% highlight python %}
predictions = model_naive.predict(X_test)
accuracy(y_test,predictions)
{% endhighlight %}        
<pre class="output">
0.562
</pre>
<p>Its seems like we have acheived a good accuracy better than random guessing.Naive bayes is not performing that well in sentiment analysis.</p>
<hr>
<h2>Random Forest Classifier</h2>

<p>As we have seen naive bayes is not giving better predictions,Lets use random forest an ensemble of Decision Trees.</p>


{% highlight python %}
from sklearn.ensemble import RandomForestClassifier
model_randomf = RandomForestClassifier(n_estimators=100)
model_randomf.fit(X_train,y_train)
{% endhighlight %}        

<p>Lets check the predictions and acuracy of our random forest model.</p>


{% highlight python %}

predictions = model_randomf.predict(X_test)
print "accuracy :",accuracy_score(y_test,predictions)
{% endhighlight %}        
<pre class="output">
accuracy : 0.6189708737864078
</pre>
<p>Whoo ! a 5% increment in accurcy using random forest.we have improved our model performance significantly using random forest.</p>

<hr>
<h2>Predictions on Raw reviews</h2>

At last we come to this section where we will be using random tweets of movie reviews to judge our model performance and to check wether our model generalizes over real data or not.

Some of the reviews are as :




</div>



</div>
{% include footer.html  %}
</div>
            </div>
        </div>
        <!-- /#page-content-wrapper -->

    </div>
<script src="../css/blog.js" type="text/javascript"></script>
</body>
</html>
